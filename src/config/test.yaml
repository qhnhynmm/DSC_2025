data:
  train_path: data/train/vihallu-train.csv
  test_path: data/test/vihallu-public-test.csv
  dev_ratio: 0.1
  processing_data: True

training:
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  num_workers: 4
  num_train_epochs: 100             # Số epoch để train
  patience: 3                      # Early stopping patience
  learning_rate: 3e-4              # Learning rate
  weight_decay: 0.01               # Weight decay cho optimizer
  gradient_accumulation_steps: 1   # Nếu muốn gradient accumulation
  output_dir: "./checkpoints"      # Thư mục lưu checkpoint
  metric_for_best_model: "accuracy"

testing:
  per_device_eval_batch_size: 32
  num_workers: 4

tokenizer:
  max_length: 512        # chiều dài tối đa input sequence
  padding: max_length    # có thể đổi thành "longest" nếu muốn tiết kiệm tính toán
  truncation: True
  return_attention_mask: True

text_embedding:
  text_encoder: "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7"  # model của SBERT, có thể đổi thành các model khác trong https://www.sbert.net/docs/pretrained_models.html
  d_model: 768                        # dimension sau projection (tùy bạn muốn giảm chiều về bao nhiêu)
  d_features: 768                     # hidden size của encoder (768 cho BERT-base, 1024 cho BERT-large)
  dropout: 0.3

  add_new_token: False                # có thêm vocab ngoài không
  freeze: False                       # True thì encoder không học, False thì fine-tune
  use_lora: True                      # bật LoRA để fine-tune nhẹ
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  lora_target_modules: ["query", "value"]   # các module áp dụng LoRA (QKV layers của Transformer)

model:
  num_labels: 3                     # số lớp phân loại

model_type: triple_classifier
